# -*- coding: utf-8 -*-
"""DataScience Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WSWu1G0P3HtGqOZWctAC_C1xomXL0LbQ
"""

import pandas as pd

data_frame = pd.read_csv("/content/covid_19_data.csv")
#data_frame.info()
#data_frame.describe()
#data_frame.shape
#data_frame.isnull()
#data_frame.isnull().sum()
#data_frame.dropna()
#data_frame.dropna(inplace=True)
#data_frame.fillna(0,inplace=True)
#data_frame.isnull().sum()
data_frame

import pandas as pd
df=data_frame.loc[1:3,['Deaths','Recovered']]
df

df2= data_frame.iloc[2:4,0:5]
df2

data_frame.head()

data_frame.tail()

df=data_frame[data_frame['Deaths']>199.0]
df
df.sort_values('Deaths',ascending=True)

import pandas as pd
import numpy as np
data = """
Id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
1,5.1,3.5,1.4,0.2,Iris-setosa
2,4.9,3,1.4,0.2,Iris-setosa
3,4.7,3.2,1.3,0.2,Iris-setosa
4,4.6,3.1,1.5,0.2,Iris-setosa
5,5,3.6,,0.2,Iris-setosa
6,5.4,3.9,1.7,0.4,Iris-setosa
7,4.6,3.4,1.4,0.3,Iris-setosa
8,5,3.4,,0.2,Iris-setosa
9,4.4,2.9,,0.2,Iris-setosa
10,4.9,3.1,1.5,0.1,Iris-setosa
# ... (rest of the data)
"""
df = pd.read_csv(r"/content/iris_data.csv")

print(df.info())
print(df.isnull().sum())
def impute_numerical(df):

  for col in df.select_dtypes(include=[np.number]).columns:
    if col != 'Species':
      df[col] = df[col].fillna(df[col].mean())
    return df


df_imputed_numerical = impute_numerical(df.copy())
def impute_forward_fill(df):
  df.fillna(method='ffill', inplace=True)  # Forward fill missing values
  return df

df_imputed_ffill = impute_forward_fill(df.copy())

from sklearn.impute import SimpleImputer

def impute_leave_one_out(df):
  imputer = SimpleImputer(strategy='mean')
  X = df.drop('Species', axis=1)
  y = df['Species']
  imputer.fit(X)
  X_transformed = imputer.transform(X)
  df_imputed_loi = pd.DataFrame(X_transformed, columns=X.columns)
  df_imputed_loi['Species'] = y
  return df_imputed_loi

try:
  df_imputed_loi = impute_leave_one_out(df.copy())
except ModuleNotFoundError:
  print("scikit-learn is not installed. Leave one out imputation cannot be performed.")

print(df.head())
print(df_imputed_numerical.head())
print(df_imputed_ffill.head())
print(df_imputed_loi.head())

import pandas as pd
from sklearn.impute import SimpleImputer as Imputer
df = pd.read_csv(r"/content/iris_data.csv")
print(df)
print(df.columns)
print(df.isnull())
print(df.isnull().sum())
print(df.shape)
d1 = Imputer(strategy="mean")
df["SepalLengthCm"] = d1.fit_transform(df[["SepalLengthCm"]]).reshape(-1,)
print(df["SepalLengthCm"].values)
d2 = Imputer(strategy="mean")
df["SepalWidthCm"] = d2.fit_transform(df[["SepalWidthCm"]]).reshape(-1,)
print(df["SepalWidthCm"].values)
d3 = Imputer(strategy="mean")
df["PetalLengthCm"] = d3.fit_transform(df[["PetalLengthCm"]]).reshape(-1,)
print(df["PetalLengthCm"].values)
print(df)
print(df.isnull().sum())

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
df = pd.read_csv("/content/titanic_data.csv")

imputer = SimpleImputer(strategy='most_frequent')
x = df.iloc[:, 1:5].values
x = imputer.fit_transform(x)

le = LabelEncoder()
y = df['class']
y = le.fit_transform(y)

scaler = StandardScaler()
X = df.iloc[:, 0:2].values
X = scaler.fit_transform(X)

print("Imputed X:")
print(x)
print("\nEncoded y:")
print(y)
print("\nStandardized X:")
print(X)

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
df=pd.read_csv("/content/titanic_data.csv")


imputer=SimpleImputer(strategy='most_frequent')
x=df.iloc[:,1:5].values
x=imputer.fit_transform(x)
x

y = df['class']
y.unique()
enc = {
    "First": 1,
    "Second": 2,
    "Third": 3
}
y = y.map(enc)
y.values

from sklearn.datasets import load_iris
iris = load_iris()
x = iris.data
y = iris.target
print(x)
print(y)

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
naive_bayes_classifier = GaussianNB()
naive_bayes_classifier.fit(X_train, y_train)

predictions=naive_bayes_classifier.predict(X_test)
predictions

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
accuracy

#20/06/2004
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

data = {
    "x":np.array([20,34,22,15,67,89,123,56,33,50]),
    "y":np.array([1,1,1,1,0,0,0,0,1,0])
}

x=data["x"].reshape(-1,1)
y=data["y"]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
predict = knn.predict([[56]])
predict

#linear regression algorithm

import pandas as pd

df=pd.read_csv("/content/CarPrice_Assignment.csv")
X=df["wheelbase"].values.reshape(-1,1)
y=df["price"].values
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

df=pd.read_csv("/content/CarPrice_Assignment.csv")
print(data.head())

X = data['horsepower'].values
y = data['price'].values

slope, intercept, r, p, std_err = stats.linregress(X, y)

def linreg(x):
    return slope * x + intercept

linModel = list(map(linreg, X))


plt.scatter(X, y, label='Data points')
plt.plot(X, linModel, color='red', label='Fitted line')
plt.xlabel('Horsepower')
plt.ylabel('Price')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

file_path = '/content/CarPrice_Assignment.csv'
data = pd.read_csv(file_path)

X = data['horsepower'].values
y = data['price'].values

slope, intercept, r, p, std_err = stats.linregress(X, y)

def linreg(x):
    return slope * x + intercept

linModel = list(map(linreg, X))


plt.scatter(X, y)
plt.plot(X, linModel)
plt.show()

import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt

# Load data from CSV
data = pd.read_csv('/content/simplelinearregression.csv')

# Print the column names to ensure they are as expected
print("Column names:", data.columns)

# Print the first few rows of the data to ensure it is loaded correctly
print("First few rows of data:\n", data.head())

# Extracting X and y values
# Ensure the correct column names are used here
X = data.iloc[:, 0].values  # Assuming X is in the first column
y = data.iloc[:, 1].values  # Assuming y is in the second column

# Print the extracted X and y values to check
print("X values:", X)
print("y values:", y)

# Calculate linear regression
slope, intercept, r, p, std_err = stats.linregress(X, y)

# Define the linear regression function
def linreg(x):
    return slope * x + intercept

# Generate the linear model values
linModel = list(map(linreg, X))

# Plot the data
plt.scatter(X, y, label='Data points')
plt.plot(X, linModel, label='Fitted line')  # Plot the fitted line without specifying a color
plt.legend()
plt.show()

#21/6/24
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

X = np.random.rand(100,1)
y= 2*X**2 + np.random.rand(100,1)
X

p_features = PolynomialFeatures(degree=3)
X_poly = p_features.fit_transform(X)
X_poly

lin_model= LinearRegression()
lin_model.fit(X_poly,y)

y_pred = lin_model.predict(X_poly)
mse = mean_squared_error(y, y_pred)
print("MSE:", mse)

plt.scatter(X, y)
plt.plot(X, y_pred,color='red')
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = pd.read_csv('/content/CarPrice_Assignment.csv')
df = pd.DataFrame(data)
df

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

log_model = LogisticRegression()
log_model.fit(X_train, y_train)

predictions = log_model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

log_model.predict([[102]])

#22/6/24

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

dt_classifier = DecisionTreeClassifier(random_state=23)
dt_classifier.fit(X_train, y_train)

predictions = dt_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

predict = dt_classifier.predict([[5.1, 3.5, 1.4, 0.2]])
predict

#decision tree regressor
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

data = pd.read_csv('/content/CarPrice_Assignment.csv')

X = data.iloc[:,1:2].values
print(X.shape)
y = data["enginesize"].values
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

dt_regressor = DecisionTreeRegressor(random_state=42)
dt_regressor.fit(X_train, y_train)

predictions = dt_regressor.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print("MSE:", mse)

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

#svm algorithm...

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

data = pd.read_csv('/content/CarPrice_Assignment.csv')

X = data.iloc[:,1:2].values
print(X.shape)
y = data["enginesize"].values
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

svm_model = SVC(kernel="linear",decision_function_shape="ovr")
svm_model.fit(X_train, y_train)

predictions = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

#pca -> it will combine the data and show it as less...it will not remove the data
#first standardizing the data

from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
y = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)
plt.colorbar(label='Target')
plt.show()

#24/6/24

!pip install pycaret

from pycaret.classification import *
import pandas as pd

df = pd.read_csv("/content/Salary_Data.csv")
df

set_1= setup(df,target="Salary")

compare_models()

extra_tress_model = create_model("et")

evaluate_model(extra_tress_model)

from pycaret.regression import *
import pandas as pd

df = pd.read_csv("/content/CarPrice_Assignment.csv")

set_2 = setup(df,target="price")

compare_models()

linear_model = create_model("lr")

evaluate_model=evaluate_model(linear_model)

predict_model(linear_model,data=df)

from pycaret.classification import*
import pandas as pd
df = pd.read_csv('/content/titanic_data.csv')
df

set_up = setup(df,target="alone",preprocess=True)

compare_models()

logistics_model=create_model("lr")

evaluate_model(logistics_model)

predict_model(logistics_model,data=df)

!pip install torch

import torch
import torch.nn as nn
import torch.optim as optim

class di_Model(nn.Module):
  def __init__(self,input_size,hidden_size,output_size):
    super(di_Model,self).__init__()
    self.fc1=nn.Linear(input_size,hidden_size)
    self.relu=nn.ReLU()
    self.fc2=nn.Linear(hidden_size,output_size)
    self.sigmoid=nn.Sigmoid()

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris=load_iris()
X=iris.data
y=iris.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

X_train=torch.tensor(X_train,dtype=torch.float32)
X_test=torch.tensor(X_test,dtype=torch.float32)
y_train=torch.tensor(y_train,dtype=torch.float32)
y_test=torch.tensor(y_test,dtype=torch.float32)

new_iris = torch.tensor([[5.1, 3.5, 1.4, 0.2]], dtype=torch.float32)
with torch.no_grad():
  print(new_iris)
  predict= model(new_iris)

!pip install pycaret

from pycaret.classification import *
import pandas as pd

df = pd.read_csv("/content/CarPrice_Assignment.csv")
df

set_up= setup(df,target="price",preprocess=True,data_split_stratify=True)

def text_processing(text):
  tokens = [token.lower() for token in text.split()]
  return tokens

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

!pip install torch

!pip install torch transformers

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

data ='suriya7/English-to-Tamil'
tokenizer = AutoTokenizer.from_pretrained(data)
model = AutoModelForSeq2SeqLM.from_pretrained(data)

def translate(text):
  tokenized_text = tokenizer(text, return_tensors="pt")
  translated_text = model.generate(**tokenized_text)
  return tokenizer.decode(translated_text[0], skip_special_tokens=True)

text="Hard to Catch"
output = translate(text)
print(output)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import pandas as pd
import string

def text_processing(text):
  text = text.lower()
  text=text.translate(str.maketrans('', '', string.punctuation))
  tokens = word_tokenize(text)
  stop_words = set(stopwords.words('english'))
  words = [token for token in tokens if token not in stop_words]
  stemmer=PorterStemmer()
  words = [stemmer.stem(word) for word in words]
  return ' '.join(words)

nltk.download('punkt')
nltk.download('stopwords')

data = ["I'm admired Watching the movie",
        "Its not that bad",
        "Its not so good",
        "I hate this movie"
        ]

preprocessed_doc = [text_processing(doc) for doc in data]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(preprocessed_doc)
print(X.toarray())

kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
print(kmeans.labels_)

!pip install torch

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from torch import Tensor
from torch.utils.data import TensorDataset, DataLoader

file_path = '/content/transactions_train.csv'
df = pd.read_csv(file_path)
df['Date'] = pd.to_datetime(df['Date'])
df.sort_values('Date', inplace=True)
data = df['Close'].values.reshape(-1, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
def create_sequences(data, seq_length):
    X = []
    y = []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)
seq_length = 60

X, y = create_sequences(scaled_data, seq_length)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

batch_size = 32
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Define the neural network model
class FeedforwardNN(nn.Module):
    def __init__(self, input_dim):
        super(FeedforwardNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 50)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(50, 25)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(25, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu1(out)
        out = self.fc2(out)
        out = self.relu2(out)
        out = self.fc3(out)
        return out

input_dim = seq_length
model = FeedforwardNN(input_dim)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 10
for epoch in range(num_epochs):
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch.view(-1, 1))
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

model.eval()
with torch.no_grad():
    predictions = model(X_test_tensor).numpy()
    predictions = scaler.inverse_transform(predictions)

y_test_scaled = scaler.inverse_transform(y_test_tensor.view(-1, 1).numpy())
mse = np.sqrt(np.mean((predictions - y_test_scaled) ** 2))
print('MSE:', mse)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
d = pd.read_csv("/content/transactions_train.csv")
string_columns = d.select_dtypes(include=['object']).columns

label_encoder = LabelEncoder()
for col in string_columns:
    d[col] = label_encoder.fit_transform(d[col])
X = d.iloc[:,0:8].values
print(X.shape)
y = d['review_score'].values
print(y.shape)
X_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
dt_reg = DecisionTreeRegressor(random_state=23)
dt_reg.fit(X_train,y_train)
pr = dt_reg.predict(x_test)
mse = mean_squared_error(y_test,pr)
print("Mean Squared Error:",mse)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Example data - assuming we're plotting temperature data
np.random.seed(0)
data = np.random.normal(loc=14, scale=2, size=1000)  # Simulated temperature data

# Create the figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Histogram (without box plot overlapping)
sns.histplot(data, kde=True, bins=30, color='skyblue', edgecolor='black', ax=ax)

# Box plot on the top of the histogram
sns.boxplot(data=data, color='orange', width=0.3, ax=ax, fliersize=5)

# Title and labels
ax.set_title('Combined Box and Histogram Plot for Climate Change Data', fontsize=16)
ax.set_xlabel('Temperature (°C)', fontsize=12)
ax.set_ylabel('Frequency', fontsize=12)

# Show the plot
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/Instagram data.csv', encoding='ISO-8859-1')

numeric_data = data.select_dtypes(include=['int64'])
numeric_sums = numeric_data.sum()

plt.figure(figsize=(10, 6))
numeric_sums.sort_values().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Total Values for Each Metric', fontsize=16)
plt.ylabel('Total', fontsize=12)
plt.xticks(rotation=45)
plt.show()

selected_columns = ['Impressions', 'Likes', 'Comments', 'Shares']
data[selected_columns].hist(bins=10, figsize=(12, 8), color='skyblue',
edgecolor='black')
plt.suptitle('Distributions of Metrics', fontsize=16)
plt.show()

sources = ['From Home', 'From Hashtags', 'From Explore', 'From Other']
avg_sources = data[sources].mean()

plt.figure(figsize=(8, 8))
plt.pie(avg_sources, labels=sources, autopct='%1.1f%%',
startangle=140, colors=sns.color_palette("pastel"))
plt.title('Average Source of Impressions', fontsize=16)
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=data['Likes'], y=data['Comments'], color='green', s=100)
plt.title('Relationship Between Likes and Comments', fontsize=16)
plt.xlabel('Likes', fontsize=12)
plt.ylabel('Comments', fontsize=12)
plt.show()


sample_data = data[['Likes', 'Shares']].head(10)
plt.figure(figsize=(10, 6))
plt.plot(sample_data['Likes'], label='Likes', marker='o', color='blue')
plt.plot(sample_data['Shares'], label='Shares', marker='o', color='orange')
plt.title('Trend of Likes and Shares (Sample)', fontsize=16)
plt.xlabel('Entry Index', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('/content/Instagram data.csv',encoding="ISO-8859-1")
data.head()
print(data.isnull().sum())
print(data.describe())
df=pd.DataFrame(data,columns=['Impressions','From Home','From Hashtags','From Explore','From Other','Saves','Comments','Shares',
                                'Likes','Profile Visits','Follows','Hashtags'])
df['Hashtags'] = df['Hashtags'].str.replace("#", "")
df.to_csv(r'Instagram_data.csv', index=False)

plt.plot(data['Impressions'].iloc[0:10],data['Likes'].iloc[0:10])
plt.xlabel('Impressions')
plt.ylabel('Likes')
plt.title('Line Chart')
plt.show()
data['Follows'].value_counts().plot(kind='bar')
plt.title('Bar Chart')
plt.show()
plt.scatter(data['Likes'],data['Profile Visits'])
plt.xlabel('Likes')
plt.ylabel('Profile Visits')
plt.title('Scatter Plot')
plt.show()
categories = ['Shares','Likes','Comments']
values = [5,162,9]
plt.pie(values, labels=categories, autopct='%1.1f%%', colors=['blue','orange', 'green'])
plt.title('People\'s Preference')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/Instagram data.csv', encoding='ISO-8859-1')

numeric_data = data.select_dtypes(include=['int64'])
numeric_sums = numeric_data.sum()

plt.figure(figsize=(8, 6))
sns.scatterplot(x=data['Likes'], y=data['Comments'], color='green', s=100)
plt.title('Relationship Between Likes and Comments', fontsize=16)
plt.xlabel('Likes', fontsize=12)
plt.ylabel('Comments', fontsize=12)
plt.show()

sample_data = data[['Likes', 'Shares']].head(10)
plt.figure(figsize=(10, 6))
plt.plot(sample_data['Likes'], label='Likes', marker='o', color='blue')
plt.plot(sample_data['Shares'], label='Shares', marker='o', color='orange')
plt.title('Trend of Likes and Shares (Sample)', fontsize=16)
plt.xlabel('Entry Index', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend()
plt.show()

data['Follows'].value_counts().plot(kind='bar')
plt.title('Bar Chart')
plt.show()

categories = ['Shares','Likes','Comments']
values = [5,162,9]
plt.pie(values, labels=categories, autopct='%1.1f%%', colors=['blue','orange', 'green'])
plt.title('People\'s Preference')
plt.show()